<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academic</title>
    <link>/tag/academic/</link>
      <atom:link href="/tag/academic/index.xml" rel="self" type="application/rss+xml" />
    <description>Academic</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 27 Nov 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>Academic</title>
      <link>/tag/academic/</link>
    </image>
    
    <item>
      <title>Co-localization analysis of fluorescence microscopy images</title>
      <link>/post/colocr-ropensci/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/post/colocr-ropensci/</guid>
      <description>&lt;p&gt;A few months ago, I wasn’t sure what to expect when looking at fluorescence microscopy images in published papers. I looked at the accompanying graph to understand the data or the point the authors were trying to make. Often, the graph represents one or more measures of the so-called co-localization, but I couldn’t figure out how to interpret them. It turned out; reading the images is simple. Cells are simultaneously stained by two dyes (say, red and green) for two different proteins. The color turns yellow in the merged image when the two proteins localize in proximity.&lt;/p&gt;
&lt;p&gt;A recent publication from our laboratory included microscopy images and a quantification graph. The standard analysis protocol is to use ImageJ with a specialized plug-in for the co-localization analysis such as coloc2 or Fiji. The images were processed one at a time; subtracting the background, selecting regions of interest and calculating the co-localization statistics. This work was done manually and repeated with minor changes in the parameters each time. At the end, there was no way to make this process reproducible, and we included only the final output in an otherwise fully reproducible article.&lt;/p&gt;
&lt;p&gt;For those two specific reasons; the need for manual work and the lack of reproducibility, I thought it would be great if I could do this analysis in R. There are several R packages for image processing such as EBImage, imager and magick. These packages provided the functionality needed for conducting this analysis. So I put the pieces together in an R package to simplify this analysis for my future self and for others who might find it useful. This is colocr, a simple R package for conducting co-localization analysis of fluorescence microscopy images. In particular, it semi-automates the selections of regions of interest-arguably the most time-consuming step- and comes with a Shiny app for visually comparing the outputs of different parameters before writing a final script.&lt;/p&gt;
&lt;h2 id=&#34;how-would-have-we-done-it-differently&#34;&gt;How would have we done it differently?&lt;/h2&gt;
&lt;p&gt;In this recent article I referred to earlier, we assessed the co-localization of several proteins with RKIP (Raf-kinas inhibitor protein) in DU145 prostate cancer cell line. For each of these proteins, we generated multiple frames, a hundred in total. We then processed the images one by one and repeated the process with each minor change in the parameters. Now we can use colocr, in just a few lines of code, to apply the same analysis.&lt;/p&gt;
&lt;p&gt;A selection of the images were presented in the article in a similar figure.&lt;/p&gt;
&lt;p&gt;image&lt;/p&gt;
&lt;p&gt;We can calculate the Pearson’s Correlation Coefficients (PCC) of multiple images at once using the following chunk of code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load required libraries
library(tidyverse)
library(colocr)

# load images
fls &amp;lt;- list.files(&#39;images&#39;, pattern = &#39;*.jpg&#39;, recursive = TRUE)

# apply co-localization test
tst &amp;lt;- image_load(fls) %&amp;gt;%
  roi_select(threshold = 90, n = 3) %&amp;gt;%
  roi_test()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the results can be visualized in a graph.&lt;/p&gt;
&lt;p&gt;image&lt;/p&gt;
&lt;p&gt;The graph shows the co-localization statistics (PCC) for RKIP and six other proteins (x-axis). The pixel intensity of the channel of each protein (red) and RKIP (green) in three regions of interest from multiple images were used to calculate the PCC. In all cases, the correlations were above 0.75 with reasonable variance. This means a high degree of co-localization between RKIP and the six target proteins.&lt;/p&gt;
&lt;p&gt;The following code is to generate the graph.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load required libraries
library(tidyverse)

# make the graph of calculated correlations
names(tst) &amp;lt;- str_split(fls, pattern = &#39;/&#39;, simplify = TRUE)[, 1]

bind_rows(tst, .id = &#39;protein&#39;) %&amp;gt;%
  group_by(protein) %&amp;gt;%
  mutate(ave = mean(pcc),
         sd = sd(pcc),
         upper = ave + sd,
         lower = ave - sd) %&amp;gt;%
  ggplot(aes(x = protein, y = pcc)) +
  geom_jitter() +
  geom_point(aes(y = ave), color = &#39;red&#39;) +
  geom_linerange(aes(ymax = upper, ymin = lower), color = &#39;red&#39;) +
  lims(y = c(0, 1.1)) +
  labs(x = &#39;&#39;, y = &amp;quot;Pearson&#39;s Correlation Coefficient (PCC)&amp;quot;) +
  theme_bw()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While developing colocr, I focused on breaking down the steps of the analysis into intuitive pieces. Hopefully, in a way that the potential user, or “myself in a few months”, can understand easily. By keeping the number of the functions limited, the inputs and the outputs consistent and making things visual as much as possible, I hope using the package needs less investment by the user. I hope biologists who are not familiar with R can still find this work useful. One thing helped me organize and iterate on the code. This was developing a Shiny app as an interface to the package.&lt;/p&gt;
&lt;h2 id=&#34;developing-a-shiny-app-in-an-r-package&#34;&gt;Developing a Shiny app in an R package&lt;/h2&gt;
&lt;p&gt;At first, I thought of adding a Shiny app as an extra feature for users who might need it. I quickly realized that for this use case, selecting regions of interest might require trial-and-error on the side of the user to arrive at suitable parameters for the selection. Having to develop the app forced me to think about organizing the functions, their input and output in a certain way. Namely, mapping the functions to the steps of the analysis that the user has to think about and making visual output a priority. Finally, testing the app exposed errors in the intermediates which might have been buried in otherwise seemingly fine R objects. Overall, I learned a thing or two about writing Shiny apps to go with R packages when possible even if it won’t make the cut in the release.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I found it useful to work on the app as early as possible and improving it side by side with the core functionality. As explained earlier, this helped me to think about the inputs and the output from a user’s perspective.&lt;/li&gt;
&lt;li&gt;I learnt to isolate the source code for the app from that of the package functions as much as possible. One may host the app code in a separate repository and include it in the project as a git submodule. Some might find this necessary, but I found it easier this way to keep track of the changes in the app and testing it on Travis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;submitting-r-packages-for-reviewing-by-ropensci&#34;&gt;Submitting R packages for reviewing by rOpenSci&lt;/h2&gt;
&lt;p&gt;colocr was my second submission to rOpenSci. My first submission was a package called cRegulome. In both cases, I had positive experiences. So I would like to thank the rOpenSci editor, the reviewers (Hao Zhu, Sean Hughes) and Jeroen Ooms who helped us to resolve issues related to running the package on Windows. At a risk of an understatement, here are three benefits I got from submitting my packages to rOpenSci which others might find encouraging doing the same.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reviewing the source could at an early stage help to spot mistakes and introducing improvement to the code that might escape the authors themselves.&lt;/li&gt;
&lt;li&gt;Often the editor and the reviewers are experts in R and the subject matter, so they can offer ideas and suggestion to improve the design of the package. Adding to being an early test users who provided useful insights.&lt;/li&gt;
&lt;li&gt;Writing and distributing code might not be everyone’s strongest skill. And certain aspects of it might be challenging. The rOpenSci team provide support during the process I found encouraging.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;things-to-improve&#34;&gt;Things to improve&lt;/h2&gt;
&lt;p&gt;I am working on improving the performance and adding more features. I welcome all contributions. Two areas of improvement are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Change dependency from imager to magick to take advantage of the well tested magick classes and functionality. issue #4.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add a feature to allow for adjusting for artifacts in the images such as luminescence, which might affect the final calculations. issue #5.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;other-resources&#34;&gt;Other resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;colocr source code on GitHub, here&lt;/li&gt;
&lt;li&gt;The Shiny app, here&lt;/li&gt;
&lt;li&gt;The package vignette, here&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Report in Confidence</title>
      <link>/post/report-medium/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/post/report-medium/</guid>
      <description>&lt;p&gt;Scientists are skeptical by nature. A scientist would pay all effort while formulating a hypothesis, setting assumptions, collecting data, analyzing it, and reporting results. Sometimes in practice, this chain of careful skeptical steps breaks, and at certain steps more than the others. Scientific literature might be less attentive when quantifying and reporting uncertainty associated with scientific results. This could be due to the rush or the lack of a proper understanding of the statistical methods. And more often, due to the over usage of some convenient methods but not others.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;p-value&lt;/em&gt; ignores the effect size and gets smaller as a consequence of increasing the sample size&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;p-values&lt;/em&gt; dominate the scientific literature as a statistical summary, reported at least once in 96% of the literature (Kyriacou 2016)⁠. a &lt;em&gt;p-value&lt;/em&gt; is a probability of observing data as extreme as a test statistics when the null hypothesis is true. When this probability is very small, smaller than a predefined value, we can reject the &lt;em&gt;null&lt;/em&gt; Hypothesis and consider the &lt;em&gt;alternative&lt;/em&gt; true with a certain power.&lt;/p&gt;
&lt;p&gt;One major flaw in reporting &lt;em&gt;p-value&lt;/em&gt; as a sole statistical summary is that it ignores the effect size. That is the difference among averages of, say, two comparison groups in a study (&lt;em&gt;X&lt;/em&gt; &amp;amp; &lt;em&gt;Y&lt;/em&gt;). When used in hypothesis testing, &lt;em&gt;p-value&lt;/em&gt; is concerned about whether there is a difference between these averages in absolute value rather than the size of this difference (&lt;em&gt;Y&lt;/em&gt; — &lt;em&gt;X&lt;/em&gt;). We reject the &lt;em&gt;null&lt;/em&gt; hypothesis if (&lt;em&gt;Y&lt;/em&gt; ≠ &lt;em&gt;X&lt;/em&gt;) and we fail to reject if (&lt;em&gt;Y&lt;/em&gt; =&lt;em&gt;X&lt;/em&gt;) regardless the size of (&lt;em&gt;Y&lt;/em&gt; — &lt;em&gt;X&lt;/em&gt;) is. Here, there is a missing piece of information that we usually have to look for somewhere else in a scientific report. To get a sense of the difference between the study groups, we look at other statistical summaries like averages (mean) and standard error (SE) or standard error of the mean (SEM).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;p-value&lt;/em&gt; becomes smaller as a consequence of increasing the sample size (&lt;em&gt;N&lt;/em&gt;) as its denominator includes (√&lt;em&gt;N&lt;/em&gt;). However, a smaller &lt;em&gt;p-value&lt;/em&gt; doesn’t guarantee a better approximation of the truth but rather the a lower probability of observing the &lt;em&gt;null&lt;/em&gt;. Moreover, with large samples we can detect a statistical significance of a very small size which is of no scientific meaning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The cut-off 0.05 is arbitrary and &lt;em&gt;p-value&lt;/em&gt; is random in multiple testing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The decision of whether to reject or fail to reject the &lt;em&gt;null&lt;/em&gt; is depending on a predefined cut-off or a level of significance. Usually, we use either 0.01 or 0.05. There is nothing really special about those two numbers except for they appeared in the early papers used null hypothesis significance testing (NHST).&lt;/p&gt;
&lt;p&gt;Following the definition of &lt;em&gt;p-value&lt;/em&gt; in the context of the central limit theorem (CLT), &lt;em&gt;p-value&lt;/em&gt; is the probability that a normally distributed random variable is larger, in absolute value, than the observed &lt;em&gt;t&lt;/em&gt;test. So &lt;em&gt;p-value&lt;/em&gt; itself is a random variable because it is dependent on a random variable. When applying multiple tests or measuring many features we get many &lt;em&gt;false positives&lt;/em&gt; because of this randomness. So we should use corrections like Bonferroni (Bonferroni 1936)⁠ and don’t depend on even very small &lt;em&gt;p-values.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Confidence intervals provide more information and become more accurate with increasing sample sizes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Confidence interval is a random interval with, say, a 95% probability of falling on the estimated parameter, like the average (Irizarry and Love 2015)⁠. Unlike &lt;em&gt;p-value&lt;/em&gt;, confidence interval includes the observed difference, the effect size and provides a measure of uncertainty. We can obtain the same information provided by a &lt;em&gt;p-value&lt;/em&gt; using the confidence interval. In a &lt;em&gt;t&lt;/em&gt;test, we look for whether the averages of two groups are equals or not, so when equals, the difference will be 0. A 95% confidence interval that includes 0 implies the &lt;em&gt;p-value&lt;/em&gt; should be less than 0.05.&lt;/p&gt;
&lt;p&gt;The width of the interval reflects the effect size and this gets more accurate with increasing the sample size unlike &lt;em&gt;p-values&lt;/em&gt; which become smaller when the &lt;em&gt;null&lt;/em&gt; is not true as consequence. Because confidence intervals summarize the variability of the estimated value, the average, a narrow interval means a larger effect size and &lt;em&gt;vice versa&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To sum, confidence interval provides more information than &lt;em&gt;p-value&lt;/em&gt; and gets more accurate with increasing the sample size. Therefore, we recommended using confidence interval as a statistical summary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Bonferroni, C.E. 1936. “Teoria Statistica Delle Classi E Calcolo Delle Probabilita.”&lt;em&gt;Pubblicazioni R Istit. Super. Sci. Econ. Commer&lt;/em&gt; 8: 3–62.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Irizarry, Rafael A, and Michael I Love. 2015. &lt;em&gt;Data Analysis for the Life Sciences&lt;/em&gt;. LeanPub.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kyriacou, DN. 2016. “The Enduring Evolution of the P Value.” &lt;em&gt;Jama&lt;/em&gt; 315(11): 1113–15.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
